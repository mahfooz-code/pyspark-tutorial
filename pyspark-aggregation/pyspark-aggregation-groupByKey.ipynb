{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql  import SparkSession\n",
    "spark=SparkSession.builder  \\\n",
    "    .appName(\"spark-agg\") \\\n",
    "    .master(\"local\")    \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = [('A', 7), ('A', 8), ('A', -4),\n",
    "          ('B', 3), ('B', 9), ('B', -1),\n",
    "          ('C', 1), ('C', 5)]\n",
    "rdd = spark.sparkContext.parallelize(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 7), ('A', 8), ('B', 3), ('B', 9), ('C', 1), ('C', 5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop negative values\n",
    "positives = rdd.filter(lambda x: x[1] > 0)\n",
    "positives.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groupByKey() transformation can cause out of disk problems as data is sent over the network of Spark servers and collected on the reduce workers. \n",
    "\n",
    "When the number values per key are in the thousands or millions, there might be an OOM (running out of memory) error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[19] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "# find sum and avergae per key using groupByKey()\n",
    "sum_and_avg = positives.groupByKey() \\\n",
    "    .mapValues(lambda v: (sum(v), float(sum(v))/len(v)))\n",
    "print(sum_and_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sum and average per key using reduceByKey()\n",
    "# 1. create (sum, count) per key\n",
    "sum_count = positives.mapValues(lambda v: (v, 1))\n",
    "\n",
    "# 2. aggregate (sum, count) per key\n",
    "sum_count_agg = sum_count.reduceByKey(lambda x, y:\n",
    "     (x[0]+y[0], x[1]+y[1]))\n",
    "     \n",
    "# 3. finalize sum and average per key\n",
    "sum_and_avg = sum_count_agg.mapValues(\n",
    "    lambda v: (v[0], float(v[0])/v[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
