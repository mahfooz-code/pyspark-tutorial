{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Feature Engineerign\n",
    "    Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data\n",
    "     the key steps for building and using a machine learning model:\n",
    "\n",
    "#   Gather requirements for machine learning data\n",
    "\n",
    "#   Select Data: \n",
    "    Integrate data, de-normalize it into a dataset, collect it together.\n",
    "\n",
    "#   Preprocess Data:\n",
    "    Format it, clean it, sample it so you can work with it (this step is a pre or part of Feature Engineering)\n",
    "\n",
    "#   Transform Data: \n",
    "    Feature Engineering happens here.\n",
    "\n",
    "#   Model Data:\n",
    "    \n",
    "    1) Create training data (split data into “training” and “test” data sets)\n",
    "    2) Use training data to create models\n",
    "    3) Evaluate models (using test data sets) and tune them.\n",
    "\n",
    "#   Use query data and built model to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"pysark-ml-feature\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark API covers algorithms for working with features, which are roughly divided into these groups:\n",
    "\n",
    "Extraction: Extracting features from “raw” data\n",
    "\n",
    "Transformation: Scaling, converting, or modifying features\n",
    "\n",
    "Selection: Selecting a subset from a larger set of features\n",
    "\n",
    "Locality Sensitive Hashing (LSH): This class of algorithms combines aspects of feature transformation with other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Adding a new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|emp_id|salary|\n",
      "+------+------+\n",
      "|   100|120000|\n",
      "|   200|170000|\n",
      "|   300|150000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"emp_id\", \"salary\"]\n",
    "records = [(100, 120000), (200, 170000), (300, 150000)]\n",
    "df = spark.createDataFrame(records, column_names)\n",
    "df.show()\n",
    "df2 = df.withColumn(\"bonus\", df.salary * 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+\n",
      "|emp_id|salary|tripled_col|\n",
      "+------+------+-----------+\n",
      "|   100|120000|     360000|\n",
      "|   200|170000|     510000|\n",
      "|   300|150000|     450000|\n",
      "+------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(\"integer\")\n",
    "def tripled(num):\n",
    "    return 3*int(num)\n",
    "\n",
    "df2 = df.withColumn('tripled_col', tripled(df.salary))\n",
    "df2.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
