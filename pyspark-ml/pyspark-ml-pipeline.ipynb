{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "\n",
    "A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer.\n",
    "When Pipeline.fit() is called, the stages are executed in order. \n",
    "\n",
    "If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model.\n",
    "Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. \n",
    "\n",
    "If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage.\n",
    "\n",
    "The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "\n",
    "If stages is an empty list, the pipeline acts as an identity transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"pysark-ml-pipeline\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 'CS', 'MS'),\n",
    "    (2, 'MATH', 'PHD'),\n",
    "    (3, 'MATH', 'MS'),\n",
    "    (4, 'CS', 'MS'),\n",
    "    (5, 'CS', 'PHD'),\n",
    "    (6, 'ECON', 'BS'),\n",
    "    (7, 'ECON', 'BS'),\n",
    "], ['id', 'dept', 'education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage-1 : transform the `dept` column to numeric\n",
    "stage_1 = StringIndexer(inputCol= 'dept', outputCol= 'dept_index')\n",
    "\n",
    "# Stage-2 : transform the `education` column to numeric\n",
    "stage_2 = StringIndexer(inputCol= 'education', outputCol= 'education_index')\n",
    "\n",
    "# Stage-3 : one hot encode the numeric column `education_index`\n",
    "stage_3 = OneHotEncoder(inputCols=['education_index'], outputCols=['education_OHE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup the pipeline: glue the stages together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit the pipeline model and transform the data as defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# view the transformed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+----------+---------------+-------------+\n",
      "|id |dept|education|dept_index|education_index|education_OHE|\n",
      "+---+----+---------+----------+---------------+-------------+\n",
      "|1  |CS  |MS       |0.0       |0.0            |(2,[0],[1.0])|\n",
      "|2  |MATH|PHD      |2.0       |2.0            |(2,[],[])    |\n",
      "|3  |MATH|MS       |2.0       |0.0            |(2,[0],[1.0])|\n",
      "|4  |CS  |MS       |0.0       |0.0            |(2,[0],[1.0])|\n",
      "|5  |CS  |PHD      |0.0       |2.0            |(2,[],[])    |\n",
      "|6  |ECON|BS       |1.0       |1.0            |(2,[1],[1.0])|\n",
      "|7  |ECON|BS       |1.0       |1.0            |(2,[1],[1.0])|\n",
      "+---+----+---------+----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = pipeline_model.transform(df)\n",
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarizer\n",
    "Binarize data means to set feature values to 0 or 1 according to a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "raw_df = spark.createDataFrame([\n",
    "    (1, 0.1),\n",
    "    (2, 0.2),\n",
    "    (3, 0.5),\n",
    "    (4, 0.8),\n",
    "    (5, 0.9),\n",
    "    (6, 1.1)\n",
    "], [\"id\", \"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n"
     ]
    }
   ],
   "source": [
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_df = binarizer.transform(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------------+\n",
      "|id |feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|1  |0.1    |0.0              |\n",
      "|2  |0.2    |0.0              |\n",
      "|3  |0.5    |0.0              |\n",
      "|4  |0.8    |1.0              |\n",
      "|5  |0.9    |1.0              |\n",
      "|6  |1.1    |1.0              |\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binarized_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
