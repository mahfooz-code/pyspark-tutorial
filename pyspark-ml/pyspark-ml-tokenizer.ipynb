{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is an algorithm (or set of algorithms) for splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words, bigrams, or terms.\n",
    "\n",
    "Each of these smaller units are called tokens. For example, the lexical analyzer (as an algorithm in compiler writing) breaks programming syntaxes into a series of tokens, by removing any whitespace or comments in the source code.\n",
    "\n",
    "Therefore, tokenization is a process of spliting a string into words, symbols, or any other meaningful tokens (such as bigram or N-grams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"pyspark-ml-tokenizer\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "|id |text                 |\n",
      "+---+---------------------+\n",
      "|1  |a Fox jumped over FOX|\n",
      "|2  |RED of fox jumpled   |\n",
      "+---+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [(1, \"a Fox jumped over FOX\"),\n",
    "            (2, \"RED of fox jumpled\")]\n",
    "df = spark.createDataFrame(docs, [\"id\", \"text\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------------+-------------+\n",
      "|text                 |tokens                     |tokens_length|\n",
      "+---------------------+---------------------------+-------------+\n",
      "|a Fox jumped over FOX|[a, fox, jumped, over, fox]|5            |\n",
      "|RED of fox jumpled   |[red, of, fox, jumpled]    |4            |\n",
      "+---------------------+---------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer.transform(df)\n",
    "tokenized.select(\"text\", \"tokens\").withColumn(\"tokens_length\", countTokens(col(\"tokens\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------------+-------------+\n",
      "|text                 |tokens                  |tokens_length|\n",
      "+---------------------+------------------------+-------------+\n",
      "|a Fox jumped over FOX|[fox, jumped, over, fox]|4            |\n",
      "|RED of fox jumpled   |[red, fox, jumpled]     |3            |\n",
      "+---------------------+------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\",\n",
    "pattern=\"\\\\W\", minTokenLength=3)\n",
    "regex_tokenized = regexTokenizer.transform(df)\n",
    "regex_tokenized.select(\"text\", \"tokens\").withColumn(\"tokens_length\", countTokens(col(\"tokens\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|id |text                          |\n",
      "+---+------------------------------+\n",
      "|1  |a Fox jumped, over, the fence?|\n",
      "|2  |a RED, of fox?                |\n",
      "+---+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [(1, \"a Fox jumped, over, the fence?\"),\n",
    "            (2, \"a RED, of fox?\")]\n",
    "df = spark.createDataFrame(docs, [\"id\", \"text\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+----------------------------------+--------------------+\n",
      "|id |text                          |text2                             |text3               |\n",
      "+---+------------------------------+----------------------------------+--------------------+\n",
      "|1  |a Fox jumped, over, the fence?|[a, fox, jumped, over, the, fence]|[fox, jumped, fence]|\n",
      "|2  |a RED, of fox?                |[a, red, of, fox]                 |[red, fox]          |\n",
      "+---+------------------------------+----------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "tk = RegexTokenizer(pattern=r'(?:\\p{Punct}|\\s)+', inputCol=\"text\", outputCol='text2')\n",
    "sw = StopWordsRemover(inputCol='text2', outputCol='text3')\n",
    "pipeline = Pipeline(stages=[tk, sw])\n",
    "df4 = pipeline.fit(df).transform(df)\n",
    "df4.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
