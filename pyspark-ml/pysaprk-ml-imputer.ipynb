{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Imputer \n",
    "\n",
    "It is an imputation transformer for completing missing values.\n",
    "Real world datasets may contain missing values, often encoded as nulls, blanks, NaNs or other placeholders (for example in SQL, missing values are denoted by NULL).\n",
    "There are many methods to handle missing values:\n",
    "\n",
    "1) Delete instances if there is any missing feature (this might not be such a good idea since important information from other features will be lost)\n",
    "\n",
    "2) For a missing feature, find the average of that feature and replace missing values by the computed average\n",
    "\n",
    "3) A better strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n",
    "\n",
    "Imputer uses either the mean or the median of the columns in which the missing values are located.\n",
    "The input columns should be of numeric type.\n",
    "\n",
    "Currently Imputer does not support categorical features and may create incorrect values for a categorical feature.\n",
    "\n",
    "Note that the mean/median/mode value is computed after filtering out missing values. All null values in the input columns are treated as missing, and so are also imputed. For computing median, pyspark.sql.DataFrame.approxQuantile() is used with a relative error of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"pysark-ml-imputer\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "|id |col1|col2|\n",
      "+---+----+----+\n",
      "|1  |12.0|5.0 |\n",
      "|2  |7.0 |10.0|\n",
      "|3  |10.0|12.0|\n",
      "|4  |5.0 |NaN |\n",
      "|5  |6.0 |null|\n",
      "|6  |NaN |NaN |\n",
      "|7  |null|null|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 12.0, 5.0),\n",
    "    (2, 7.0, 10.0),\n",
    "    (3, 10.0, 12.0),\n",
    "    (4, 5.0, float(\"nan\")),\n",
    "    (5, 6.0, None),\n",
    "    (6, float(\"nan\"), float(\"nan\")),\n",
    "    (7, None, None)\n",
    "    ], [\"id\", \"col1\", \"col2\"])\n",
    "    \n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+\n",
      "|id |col1|col2|col1_out|col2_out|\n",
      "+---+----+----+--------+--------+\n",
      "|1  |12.0|5.0 |12.0    |5.0     |\n",
      "|2  |7.0 |10.0|7.0     |10.0    |\n",
      "|3  |10.0|12.0|10.0    |12.0    |\n",
      "|4  |5.0 |NaN |5.0     |9.0     |\n",
      "|5  |6.0 |null|6.0     |9.0     |\n",
      "|6  |NaN |NaN |8.0     |9.0     |\n",
      "|7  |null|null|8.0     |9.0     |\n",
      "+---+----+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(inputCols=[\"col1\", \"col2\"], outputCols=[\"col1_out\", \"col2_out\"])\n",
    "model = imputer.fit(df)\n",
    "transformed = model.transform(df)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+\n",
      "|id |col1|col2|col1_out|col2_out|\n",
      "+---+----+----+--------+--------+\n",
      "|1  |12.0|5.0 |12.0    |5.0     |\n",
      "|2  |7.0 |10.0|7.0     |10.0    |\n",
      "|3  |10.0|12.0|10.0    |12.0    |\n",
      "|4  |5.0 |NaN |5.0     |10.0    |\n",
      "|5  |6.0 |null|6.0     |10.0    |\n",
      "|6  |NaN |NaN |7.0     |10.0    |\n",
      "|7  |null|null|7.0     |10.0    |\n",
      "+---+----+----+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.setStrategy(\"median\")\n",
    "model = imputer.fit(df)\n",
    "transformed = model.transform(df)\n",
    "transformed.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
