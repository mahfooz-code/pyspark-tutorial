{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "One of the most popular techniques for scaling numerical data prior to modeling is standardization.\n",
    "\n",
    "Standardizing a dataset involves rescaling the distribution of values so that the mean of observed values (as a feature) is 0.00 and the standard deviation is 1.00.\n",
    "\n",
    "In general, many machine learning algorithms perform better — you can build a better realistic model — when numerical input variables — also called features — are scaled to a standard range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value is standardized as follows:\n",
    "\n",
    "$ y = (x – mean) / {std} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"pyspark-ml-standardization\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [('alex', 1), ('bob', 3), ('ali', 6), ('dave', 10)]\n",
    "columns = (\"name\", \"age\")\n",
    "samples = spark.createDataFrame(features, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|alex|  1|\n",
      "| bob|  3|\n",
      "| ali|  6|\n",
      "|dave| 10|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----+---+-------------------+\n",
      "|mean_age|stddev_age        |name|age|age_scaled         |\n",
      "+--------+------------------+----+---+-------------------+\n",
      "|5.0     |3.9157800414902435|alex|1  |-1.0215078369104984|\n",
      "|5.0     |3.9157800414902435|bob |3  |-0.5107539184552492|\n",
      "|5.0     |3.9157800414902435|ali |6  |0.2553769592276246 |\n",
      "|5.0     |3.9157800414902435|dave|10 |1.276884796138123  |\n",
      "+--------+------------------+----+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev, mean, col\n",
    "\n",
    "(samples.select(mean(\"age\").alias(\"mean_age\"), \n",
    "    stddev(\"age\").alias(\"stddev_age\")\n",
    "    )\n",
    ".crossJoin(samples)\n",
    ".withColumn(\"age_scaled\" , (col(\"age\") - col(\"mean_age\")) / col(\"stddev_age\"))\n",
    ") \\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------------+\n",
      "|name|age|age_scaled         |\n",
      "+----+---+-------------------+\n",
      "|alex|1  |-1.0215078369104984|\n",
      "|bob |3  |-0.5107539184552492|\n",
      "|ali |6  |0.2553769592276246 |\n",
      "|dave|10 |1.276884796138123  |\n",
      "+----+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_age, sttdev_age = samples.select(mean(\"age\"), stddev(\"age\")).first()\n",
    "samples.withColumn(\"age_scaled\", (col(\"age\") - mean_age) / sttdev_age).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Assembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|name|age|age_vector|\n",
      "+----+---+----------+\n",
      "|alex|  1|     [1.0]|\n",
      "| bob|  3|     [3.0]|\n",
      "| ali|  6|     [6.0]|\n",
      "|dave| 10|    [10.0]|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=['age'], outputCol=\"age_vector\")\n",
    "samples2 = vecAssembler.transform(samples)\n",
    "samples2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+---------------------+\n",
      "|name|age|age_vector|age_scaled           |\n",
      "+----+---+----------+---------------------+\n",
      "|alex|1  |[1.0]     |[-1.0215078369104984]|\n",
      "|bob |3  |[3.0]     |[-0.5107539184552492]|\n",
      "|ali |6  |[6.0]     |[0.2553769592276246] |\n",
      "|dave|10 |[10.0]    |[1.276884796138123]  |\n",
      "+----+---+----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"age_vector\", outputCol=\"age_scaled\",\n",
    "  withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(samples2)\n",
    "scaledData = scalerModel.transform(samples2)\n",
    "scaledData.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
