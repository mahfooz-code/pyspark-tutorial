{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join\n",
    "T1.join(T2) = {(k, (v1, v2))}\n",
    "\n",
    "T2.join(T1) = {(k, (v2, v1))}\n",
    "\n",
    "where k = k1 = k2,\n",
    "\n",
    "(k, v1) in T1,\n",
    "\n",
    "(k, v2) in T2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"pyspark-dataframe-join\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id| v1|\n",
      "+---+---+\n",
      "|  a| 10|\n",
      "|  a| 11|\n",
      "|  a| 12|\n",
      "|  b|100|\n",
      "|  b|200|\n",
      "|  c| 80|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = [('a', 10), ('a', 11), ('a', 12), ('b', 100), ('b', 200), ('c', 80)]\n",
    "T1 = spark.createDataFrame(d1, ['id', 'v1'])\n",
    "T1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id| v2|\n",
      "+---+---+\n",
      "|  a| 40|\n",
      "|  a| 50|\n",
      "|  b|300|\n",
      "|  b|400|\n",
      "|  d| 90|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2 = [('a', 40), ('a', 50), ('b', 300), ('b', 400), ('d', 90)]\n",
    "T2 = spark.createDataFrame(d2, ['id', 'v2'])\n",
    "T2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|id |v1 |id |v2 |\n",
      "+---+---+---+---+\n",
      "|a  |10 |a  |40 |\n",
      "|a  |10 |a  |50 |\n",
      "|a  |11 |a  |40 |\n",
      "|a  |11 |a  |50 |\n",
      "|a  |12 |a  |40 |\n",
      "|a  |12 |a  |50 |\n",
      "|b  |100|b  |300|\n",
      "|b  |100|b  |400|\n",
      "|b  |200|b  |300|\n",
      "|b  |200|b  |400|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = T1.join(T2, (T1.id == T2.id))\n",
    "joined.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# left join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# right join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join in MapReduce\n",
    "\n",
    "Assume to have two relations: R(K, B) and S(K, C) where K is a common key between relations R and S. Also assume that B and C represents attributes of R and S respectively. How do we find the join of R and S? The goal of join operation is find tuples that agree on their key K. A MapReduce implementation of Natural Join for R and S can implemented as:\n",
    "\n",
    "Map:\n",
    "For a tuple (k, b) in R emit a (key, value) pair as (k, (\"R\", b))\n",
    "\n",
    "For a tuple (k, c) in S, emit a (key, value) pair as (k, (\"S\", c))\n",
    "\n",
    "Once mappers are done, then we can execute the reducer as:\n",
    "\n",
    "Reduce:\n",
    "If a reducer key k has value list [(\"R\", v),(\"S\", w)], then emit a single (key, value) pair as (k, (v, w)). Note that join(R, S) will produce (k, (v, w)), while join(S, R) will produce (k, (w, v)).\n",
    "\n",
    "If a reducer key k has value list [(\"R\", v1), (\"R\", v2), (\"S\", w1), (\"S\", w2)], then we will emit four (key, value) pairs as\n",
    "\n",
    "(k, (v1, w1))\n",
    "(k, (v1, w2))\n",
    "(k, (v2, w1))\n",
    "(k, (v2, w2))\n",
    "Therefore, to perform a natural join between two relations R and S, we need to perform 2 map functions and one reducer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Phase\n",
    "\n",
    "1.\n",
    "key: relation R\n",
    "value: (k, b) tuple in R\n",
    "\n",
    "map(key, value) {\n",
    "\n",
    "  emit(k, (\"R\", b))\n",
    "  \n",
    "}\n",
    "\n",
    "2.\n",
    "key: relation S\n",
    "\n",
    "value: (k, c) tuple in S\n",
    "\n",
    "map(key, value) {\n",
    "\n",
    "  emit(k, (\"S\", c))\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "The output of mappers (as input to “sort and shuffle” phase) will be:\n",
    "\n",
    "(k1, \"R\", r1)\n",
    "\n",
    "(k1, \"R\", r2)\n",
    "\n",
    "...\n",
    "\n",
    "(k1, \"S\", s1)\n",
    "\n",
    "(k1, \"S\", s2)\n",
    "\n",
    "...\n",
    "\n",
    "(k2, \"R\", r3)\n",
    "\n",
    "(k2, \"R\", r4)\n",
    "\n",
    "...\n",
    "\n",
    "(k2, \"S\", s3)\n",
    "\n",
    "(k2, \"S\", s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducer Phase\n",
    "\n",
    "Before, we write a reducer function, we need to understand the magic of MapReduce, which is called a “sort and shuffle” phase (similar to SQL’s GROUP BY function).\n",
    "\n",
    "Once all mappers are done, the \"sort and shuffle\" phase will be applied to the output of mappers and it will produce input for reducers.\n",
    "\n",
    "The output of “sort and shuffle” phase will be:\n",
    "\n",
    "(k1, [(\"R\", r1), (\"R\", r2), ..., (\"S\", s1), (\"S\", s2), ...]\n",
    "\n",
    "(k2, [(\"R\", r3), (\"R\", r4), ..., (\"S\", s3), (\"S\", s4), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer function is presented below: per key k, we build 2 lists: \n",
    "\n",
    "list_R (which will hold R values/attributes)\n",
    "\n",
    "list_S (which will hold S values/attributes)\n",
    "\n",
    "then we perform a cartesian product of list_R and list_S to find the join tuples.\n",
    "\n",
    "\n",
    "#key: a unique key\n",
    "#values: [(relation, attrs)] where relation in {\"R\", \"S\"}\n",
    "#and  attrs are the relation attributes\n",
    "\n",
    "map(key, values) {\n",
    "\n",
    "  list_R = []\n",
    "\n",
    "  list_S = []\n",
    "\n",
    "  for (tuple in values) {\n",
    "\n",
    "    relation = tuple[0]\n",
    "\n",
    "    attributes = tuple[1]\n",
    "\n",
    "    if (relation == \"R\") {\n",
    "\n",
    "       list_R.append(attributes)\n",
    "\n",
    "    }\n",
    "\n",
    "    else {\n",
    "\n",
    "       list_S.append(attributes)\n",
    "\n",
    "    }\n",
    "\n",
    "  }\n",
    "\n",
    "  if (len(list_R) == 0) OR (len(list_S) == 0) {\n",
    "\n",
    "     # no common key\n",
    "     return\n",
    "\n",
    "  }\n",
    "\n",
    "  #len(list_R) > 0 AND len(list_S) > 0\n",
    "  #perform cartesian product of list_R and list_S\n",
    "  for (r in list_R) {\n",
    "\n",
    "    for (s in list_S) {\n",
    "\n",
    "       emit(key, (r, s))\n",
    "\n",
    "    }\n",
    "    \n",
    "  }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = [('a', 10), ('a', 11), ('a', 12), ('b', 100), ('b', 200), ('c', 80)]\n",
    "d2 = [('a', 40), ('a', 50), ('b', 300), ('b', 400), ('d', 90)]\n",
    "T1 = spark.sparkContext.parallelize(d1)\n",
    "T2 = spark.sparkContext.parallelize(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we map these RDDs to include the name of the relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_mapped = T1.map(lambda x: (x[0], (\"T1\", x[1])))\n",
    "t2_mapped = T2.map(lambda x: (x[0], (\"T2\", x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a reduction of the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined  = t1_mapped.union(t2_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the groupByKey() transformation on a single combined data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped  = combined.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finally perform a cartesian product for the values of each grouped entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry[0]: key\n",
    "# entry[1]: values as:\n",
    "# [(\"T1\", t11), (\"T1\", t12), ..., (\"T2\", t21), (\"T2\", t22), ...]\n",
    "\n",
    "def cartesian_product(entry):\n",
    "  T1 = []\n",
    "  T2 = []\n",
    "  key = entry[0]\n",
    "  values = entry[1]\n",
    "  for tuple in values:\n",
    "    relation = tuple[0]\n",
    "    attributes = tuple[1]\n",
    "    if (relation == \"T1\"): T1.append(attributes)\n",
    "    else: T2.append(attributes)\n",
    "  #end-for\n",
    "\n",
    "  if (len(T1) == 0) or (len(T2) == 0):\n",
    "     # no common key\n",
    "     return []\n",
    "\n",
    "\n",
    "  # len(T1) > 0 AND len(T2) > 0\n",
    "  joined_elements = []\n",
    "  # perform cartesian product of T1 and T2\n",
    "  for v in T1:\n",
    "    for w in T2:\n",
    "       joined_elements.append((key, (v, w)))\n",
    "\n",
    "    #end-for\n",
    "  #end-for\n",
    "  return joined_elements\n",
    "#end-def\n",
    "\n",
    "joined = grouped.flatMap(cartesian_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map-Side Join by RDDs\n",
    "\n",
    "I introduce a map-side join, which can reduce the cost of join between two tables.\n",
    "\n",
    "Using this pattern you can completely eliminate the need to shuffle and sort all the data to the reduce phase.\n",
    "\n",
    "Map-side join is a process where two data sets are joined by the mapper rather than using the actual join function (which can happen by combination of a mapper and reducer)\n",
    "\n",
    "What are the advantages of using map-side join? The advantages of using map side join are as follows:\n",
    "\n",
    "1. Map-side join might help in minimizing the cost that is incurred for sorting and merging in the shuffle and reduce stages.\n",
    "\n",
    "2. Map-side join might help in improving the performance of the task by decreasing the time to finish the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two tables A and B, the map-side join will be mostly suitable when table A is large and table B is a small to meduim.\n",
    "\n",
    "Since table B is not very big, then we create a hash table from B and then broadcast it to all nodes.\n",
    "\n",
    "Next, we iterate all elements of table A by a mapper and then accessing a broadcasted hash table (which denotes table B).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMP = spark.sparkContext.parallelize(\n",
    "[\n",
    "  (10, (1000, 'alex')),\n",
    "  (10, (2000, 'ted')),\n",
    "  (20, (3000, 'mat')),\n",
    "  (20, (4000, 'max')),\n",
    "  (10, (5000, 'joe'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPT= spark.sparkContext.parallelize(\n",
    "[ (10, ('ACCOUNTING', 'NEW YORK, NY')),\n",
    "  (20, ('RESEARCH', 'DALLAS, TX')),\n",
    "  (30, ('SALES', 'CHICAGO, IL')),\n",
    "  (40, ('OPERATIONS', 'BOSTON, MA')),\n",
    "  (50, ('MARKETING', 'Sunnyvale, CA')),\n",
    "  (60, ('SOFTWARE', 'Stanford, CA'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, ((1000, 'alex'), ('ACCOUNTING', 'NEW YORK, NY'))),\n",
       " (10, ((2000, 'ted'), ('ACCOUNTING', 'NEW YORK, NY'))),\n",
       " (10, ((5000, 'joe'), ('ACCOUNTING', 'NEW YORK, NY'))),\n",
       " (20, ((3000, 'mat'), ('RESEARCH', 'DALLAS, TX'))),\n",
       " (20, ((4000, 'max'), ('RESEARCH', 'DALLAS, TX')))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(EMP.join(DEPT).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will the map-side join optimize the task?\n",
    "\n",
    "Here, I assume that EMM is a large data set and DEPT is a relatively small dat set. Using map side join, to join EMP with DEPT on dept_id, we will create a brodcast varibale from a small table (this is a custom hash builder from an RDD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hash_table(dept_as_list):\n",
    "  hast_table = {}\n",
    "  for d in dept_as_list:\n",
    "    dept_id = d[0]\n",
    "    dept_name_location = d[1]\n",
    "    hast_table[dept_id] = dept_name_location\n",
    "  return hast_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_hash_table = to_hash_table(DEPT.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you may build the same hash table by using an Spark action collectAsMap(), which returns the key-value pairs in this RDD (DEPT) to the master as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_hash_table = DEPT.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using pyspark.SparkContext.broadcast, you can broadcast a read-only variable (dept_hash_table) to the Spark cluster, which will be available to all kinds of transformations (including mappers and reducers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "hash_table_broadcasted = sc.broadcast(dept_hash_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I show the map side join by using a map() transformation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_side_join(e):\n",
    "  dept_id = e[0]\n",
    "  # get hash_table from broadcasted object\n",
    "  hash_table = hash_table_broadcasted.value\n",
    "  dept_name_location = hash_table[dept_id]\n",
    "  return (e, dept_name_location)\n",
    "  \n",
    "joined = EMP.map(map_side_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, with map side join, you can avoid shuffling which can cost a lot and you avoid significant network I/O.\n",
    "With a map-side join, we just used a map() function for each row of the EMP table, and retrieved dimension values (such as dept_name and dept_location) from the broadcasted hash table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If EMP table is very large, the map() function will be executed concurrently for each partition that has own copy of hash table.\n",
    "The map side join approach allows us not to shuffle the fact table (i.e., DEPT) and to get quite good join performance.\n",
    "\n",
    "Advantages of using map-side join\n",
    "The true join cost can be reduced since we are minimizing the cost that is incurred for sorting and merging in the shuffle and reduce stages.\n",
    "\n",
    "Map-side join improves the performance of the join task by decreasing the time to finish the join operation.\n",
    "\n",
    "Disadvantages of map-side join\n",
    "Map-side join data pattern is proper when one of the RDDs/tables on which you perform map-side join operation is small enough to fit into the memory.\n",
    "\n",
    "If both RDDs/tables are not small at all, then map-side join is not suitable to perform map-side join on the RDDs/tables."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
