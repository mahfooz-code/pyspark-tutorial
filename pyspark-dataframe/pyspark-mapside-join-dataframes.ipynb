{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAp side join\n",
    "\n",
    "Map-side join makes sense when one of the tables — so called a fact table — is huge and the other table — so called a dimension table — is small enough to be broadcasted in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"spark-map-join\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achived the Joined table result, we need to do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1: Create Cache for Airports\n",
    "\n",
    "Create a broadcast varaible for airports: first we create an RDD and then save it a dictionary[(key, value)] where key is an airport code, and value is associated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_data = [\n",
    "    (\"DTW\", \"Detroit Airport\", \"Detroit\", \"MI\"),\n",
    "    (\"ORD\", \"Chicago O'Hare\", \"Chicago\",  \"IL\"),\n",
    "    (\"JFK\", \"John F. Kennedy Int. Airport\", \"New York\", \"NY\"),\n",
    "    (\"LAX\", \"Los Angeles Int. Airport\", \"Los Angeles\", \"CA\"),\n",
    "    (\"SEA\", \"Seattle-Tacoma Int. Airport\", \"Seattle\", \"WA\"),\n",
    "    (\"SFO\", \"San Francisco Int. Airport\", \"San Francisco\", \"CA\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_rdd = spark.sparkContext.parallelize(airports_data)\\\n",
    ".map(lambda tuple4: (tuple4[0], (tuple4[1],tuple4[2],tuple4[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_dict = airports_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DTW': ('Detroit Airport', 'Detroit', 'MI'),\n",
       " 'ORD': (\"Chicago O'Hare\", 'Chicago', 'IL'),\n",
       " 'JFK': ('John F. Kennedy Int. Airport', 'New York', 'NY'),\n",
       " 'LAX': ('Los Angeles Int. Airport', 'Los Angeles', 'CA'),\n",
       " 'SEA': ('Seattle-Tacoma Int. Airport', 'Seattle', 'WA'),\n",
       " 'SFO': ('San Francisco Int. Airport', 'San Francisco', 'CA')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_cache = spark.sparkContext.broadcast(airports_dict)\n",
    "airports_cache.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-2: Create Cache for Airlines\n",
    "\n",
    "Create a broadcast varaible for airlines: first we create an RDD and then save it a dictionary[(key, value)] where key is an airline code, and value is the name of airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_data = [\n",
    "    (\"SW\", \"Southwest Airlines\"),\n",
    "    (\"AA\", \"American Airlines\"),\n",
    "    (\"DL\", \"Delta Airlines\"),\n",
    "    (\"VX\", \"Virgin America\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_rdd = spark.sparkContext.parallelize(airlines_data)\\\n",
    ".map(lambda tuple2: (tuple2[0], tuple2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_dict = airlines_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SW': 'Southwest Airlines',\n",
       " 'AA': 'American Airlines',\n",
       " 'DL': 'Delta Airlines',\n",
       " 'VX': 'Virgin America'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_cache = spark.sparkContext.broadcast(airlines_dict)\n",
    "airlines_cache\n",
    "airlines_cache.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-3: Create Facts Table — Flights\n",
    "\n",
    "Map each reacord of the Flights table and perform a simple join by lookup dictionaries created in Steps 1 & 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_data = [\n",
    "    (\"DTW\", \"ORD\", \"SW\", \"225\",  \"17:10\"),\n",
    "    (\"DTW\", \"JFK\", \"SW\", \"355\",  \"8:20\"),\n",
    "    (\"SEA\", \"JFK\", \"DL\", \"418\",  \"7:00\"),\n",
    "    (\"SFO\", \"LAX\", \"AA\", \"1250\", \"7:05\"),\n",
    "    (\"SFO\", \"JFK\", \"VX\", \"12\",   \"7:05\"),\n",
    "    (\"JFK\", \"LAX\", \"DL\", \"424\",  \"7:10\"),\n",
    "    (\"LAX\", \"SEA\", \"DL\", \"5737\", \"7:10\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_colums = [\"from\", \"to\", \"airline\", \"flight_number\", \"departure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.createDataFrame(flights_data, flight_colums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------+-------------+---------+\n",
      "|from|to |airline|flight_number|departure|\n",
      "+----+---+-------+-------------+---------+\n",
      "|DTW |ORD|SW     |225          |17:10    |\n",
      "|DTW |JFK|SW     |355          |8:20     |\n",
      "|SEA |JFK|DL     |418          |7:00     |\n",
      "|SFO |LAX|AA     |1250         |7:05     |\n",
      "|SFO |JFK|VX     |12           |7:05     |\n",
      "|JFK |LAX|DL     |424          |7:10     |\n",
      "|LAX |SEA|DL     |5737         |7:10     |\n",
      "+----+---+-------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-4: Map-Side Join:\n",
    "Map each reacord of the Flights table — known as a facts table — and perform a simple join by lookup cache dictionaries created in Steps 1 & 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airport(code):\n",
    "    return airports_cache.value[code][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_airline(code):\n",
    "    return airlines_cache.value[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_udf = udf(get_airport, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_udf = udf(get_airline, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------------+-------------+---------+\n",
      "|from_city    |to_city    |airline_name      |flight_number|departure|\n",
      "+-------------+-----------+------------------+-------------+---------+\n",
      "|Detroit      |Chicago    |Southwest Airlines|225          |17:10    |\n",
      "|Detroit      |New York   |Southwest Airlines|355          |8:20     |\n",
      "|Seattle      |New York   |Delta Airlines    |418          |7:00     |\n",
      "|San Francisco|Los Angeles|American Airlines |1250         |7:05     |\n",
      "|San Francisco|New York   |Virgin America    |12           |7:05     |\n",
      "|New York     |Los Angeles|Delta Airlines    |424          |7:10     |\n",
      "|Los Angeles  |Seattle    |Delta Airlines    |5737         |7:10     |\n",
      "+-------------+-----------+------------------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.select(\n",
    "        airport_udf(\"from\").alias(\"from_city\"),\n",
    "        airport_udf(\"to\").alias(\"to_city\"),\n",
    "        airline_udf(\"airline\").alias(\"airline_name\"),\n",
    "        \"flight_number\", \"departure\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Joins using Bloom filters"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
