{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning\n",
    "\n",
    "Partitioning is the act of dividing or partitioning; separation by the creation of a boundary that divides or keeps apart.\n",
    "\n",
    "Data partitioning is used in Spark, Amazon Athena, and Google BigQuery to improve performance of query executions.\n",
    "\n",
    "To scale-out big data solutions, data is divided into partitions that can be managed, accessed, and executed separately and in parallel.\n",
    "\n",
    "Spark splits data into smaller chunks, which are called partitions and then processes these partitions in a parallel fashion (many partitions can be processed concurrently) by using executors spanned in worker nodes.\n",
    "\n",
    "By default, Spark uses a Hash Partitioner to partition the data across different partitions.\n",
    "\n",
    "The Hash Partitioner works on the concept of using the hashcode() function. \n",
    "\n",
    "    total records: 100,000,000,000\n",
    "    number of partitions: 10,000\n",
    "    number of elements per partition: 10,000,000\n",
    "    maximum possible parallelism: 10,000\n",
    "\n",
    "Partitioning data can improve manageability, scalability, reduce contention, and optimize performance.\n",
    "\n",
    "As your physical data is distributed as partitions across the physical cluster, Spark treats each partition as a high-level logical data abstraction (such as RDDs and DataFrames) in memory (and disk if there is not a sufficient memory).\n",
    "\n",
    "Spark cluster will optimize partition access and will read the partition closest to it in the network, observing data locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"data-patition\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(iterator):\n",
    "    print(\"elements=\", list(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "rdd = sc.parallelize(numbers)\n",
    "num_partitions = rdd.getNumPartitions()\n",
    "num_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.foreachPartition(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition as Text Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\")\\\n",
    "  .csv(input_path)\\\n",
    "  .toDF('customer_id', 'year', 'transaction_id', 'transaction_value')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyspark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
