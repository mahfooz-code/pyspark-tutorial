{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle Step in Reductions\n",
    "\n",
    "Shuffling is a process of redistributing data across partitions that may or may not cause moving data across JVM processes or even over the wire (between executors on separate servers).\n",
    "\n",
    "Shuffling is the process of data transfer between stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the shuffle in general? \n",
    "\n",
    "I am going to explain the “shuffling” concept by an example. Imagine that you have a 100-nodes Spark cluster and each node has a list of (URL, frequency) pairs in a table and you want to calculate the total frequency per URL. This way you would set the “URL” as your key, and for each pair you would emit “frequency” as a value. After this you would sum up frequencies for each key (i.e., URL), which would be an answer to your question — total amount of frequencies for each unique URL. But when you store the data across the cluster, how can you sum up the values for the same key stored on different servers? The only way to do so is to make all the values for the same key be on the same server, after this you would be able to sum them up. This process is called shuffling."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
