#   What is PySpark SparkJobinfo?
    
    One of the most common questions in any PySpark interview.
    PySpark SparkJobinfo is used to gain information about the SparkJobs that are in execution.
    The code for using the SparkJobInfo is as follows: 
    
    class SparkJobInfo(namedtuple("SparkJobInfo", "jobId stageIds status ")):

#   What is PySpark SparkStageinfo?

    One of the most common question in any PySpark interview question and answers guide.
    PySpark SparkStageInfo is used to gain information about the SparkStages that are present at that time.
    The code used fo SparkStageInfo is as follows: 
    
    class SparkStageInfo(namedtuple("SparkStageInfo", "stageId currentAttemptId name numTasks unumActiveTasks","numCompletedTasks", "numFailedTasks"))

#   CAN YOU USE SPARK TO PERFORM THE ETL PROCESS?
    Yes, Spark may be used for the ETL operation as Spark supports Java, Scala, R, and Python.

#   





