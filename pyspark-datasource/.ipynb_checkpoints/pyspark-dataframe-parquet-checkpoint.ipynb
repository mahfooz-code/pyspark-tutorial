{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69fa4b7",
   "metadata": {},
   "source": [
    "# Spark parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be2f6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90ef38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query=\"CREATE EXTERNAL TABLE world_temprature( \\\n",
    "    day_month_year DATE, \\\n",
    "    temperature DOUBLE \\\n",
    "    ) \\\n",
    "    PARTITIONED BY ( \\\n",
    "    continent STRING, \\\n",
    "     country STRING,\\\n",
    "     city STRING\\\n",
    "   )\\\n",
    "   STORED AS PARQUET \\\n",
    "   LOCATION 'file:/C:/Users/malam/development/data/spark/parquet' \\\n",
    "   tblproperties (\\\"parquet.compress\\\"=\\\"SNAPPY\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0666849",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"spark-parquet\").master(\"local[*]\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7810fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('2021-02-07',97.25,'asia','india','chennai'),\n",
    "    ('2021-03-07',98.33,'asia','india','mumbai'),\n",
    "    ('2021-04-08',99.45,'asia','india','patna'),\n",
    "    ('2021-09-07',96.35,'asia','india','delhi')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49bfa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(data,[\"day_month_year\",\"temperature\",\"continent\",\"country\",\"city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f05bef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+-------+-------+\n",
      "|day_month_year|temperature|continent|country|   city|\n",
      "+--------------+-----------+---------+-------+-------+\n",
      "|    2021-02-07|      97.25|     asia|  india|chennai|\n",
      "|    2021-03-07|      98.33|     asia|  india| mumbai|\n",
      "|    2021-04-08|      99.45|     asia|  india|  patna|\n",
      "|    2021-09-07|      96.35|     asia|  india|  delhi|\n",
      "+--------------+-----------+---------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe4596",
   "metadata": {},
   "source": [
    "# Creating partition table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8be892",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path=\"file:/C:/Users/malam/development/data/spark/parquet/world_temprature_by_date\"\n",
    "from pyspark.sql.functions import year,month\n",
    "\n",
    "df.withColumn(\"year\", year(df[\"day_month_year\"]))\\\n",
    "  .withColumn(\"month\", month(df[\"day_month_year\"]))\\\n",
    "  .write\\\n",
    "  .option(\"parquet.compress\",\"SNAPPY\")\\\n",
    "  .mode(\"overwrite\")\\\n",
    "  .partitionBy(\"year\", \"month\")\\\n",
    "  .parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1ee94",
   "metadata": {},
   "source": [
    "# Reading Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a79d735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "472e032e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+-------+-------+----+-----+\n",
      "|day_month_year|temperature|continent|country|   city|year|month|\n",
      "+--------------+-----------+---------+-------+-------+----+-----+\n",
      "|    2021-02-07|      97.25|     asia|  india|chennai|2021|    2|\n",
      "|    2021-03-07|      98.33|     asia|  india| mumbai|2021|    3|\n",
      "|    2021-04-08|      99.45|     asia|  india|  patna|2021|    4|\n",
      "|    2021-09-07|      96.35|     asia|  india|  delhi|2021|    9|\n",
      "+--------------+-----------+---------+-------+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42756add",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: world_temprature_by_date; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [world_temprature_by_date], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20304/783301110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select * from world_temprature_by_date\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Table or view not found: world_temprature_by_date; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [world_temprature_by_date], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from world_temprature_by_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9801f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
