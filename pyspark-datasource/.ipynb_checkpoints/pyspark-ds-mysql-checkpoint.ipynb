{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder\\\n",
    ".config(\"spark.driver.extraClassPath\", \"C:/Users/malam/jdbc/mysql.jar\") \\\n",
    ".appName(\"pyspark-mysql\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading database details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jproperties import Properties\n",
    "  \n",
    "configs = Properties()\n",
    "with open('C:\\\\Users\\\\malam\\\\jdbc\\\\mysql.properties', 'rb') as read_prop:\n",
    "    configs.load(read_prop)\n",
    "      \n",
    "jdbc_url=configs.get(\"jdbcUrl\").data \n",
    "jdbc_user=configs.get(\"user\").data\n",
    "jdbc_password=configs.get(\"password\").data\n",
    "jdbc_driver=configs.get(\"driver\").data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_no_partitioning = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", jdbc_url)\\\n",
    "    .option(\"user\", jdbc_user)\\\n",
    "    .option(\"password\", jdbc_password)\\\n",
    "    .option(\"ssl\", \"true\")\\\n",
    "    .option(\"driver\", jdbc_driver)\\\n",
    "    .option(\"dbtable\", \"emp\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empd_id: integer (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- dept_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reader_no_partitioning.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empd_id: int, emp_name: string, dept_id: int]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_no_partitioning.cache().count()\n",
    "reader_no_partitioning.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skewed column to partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_partitioning_skewed = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", jdbc_url)\\\n",
    "    .option(\"user\", jdbc_user)\\\n",
    "    .option(\"password\", jdbc_password)\\\n",
    "    .option(\"ssl\", \"true\")\\\n",
    "    .option(\"driver\", jdbc_driver)\\\n",
    "    .option(\"partitionColumn\", \"empd_id\")\\\n",
    "    .option(\"numPartitions\", 8)\\\n",
    "    .option(\"lowerBound\", 0)\\\n",
    "    .option(\"upperBound\", 4)\\\n",
    "    .option(\"dbtable\", \"emp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partitioning_skewed = reader_partitioning_skewed.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_partitioning_skewed.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empd_id: int, emp_name: string, dept_id: int]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_partitioning_skewed.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_partitioning_unif = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", jdbc_url)\\\n",
    "    .option(\"user\", jdbc_user)\\\n",
    "    .option(\"password\", jdbc_password)\\\n",
    "    .option(\"ssl\", \"true\")\\\n",
    "    .option(\"driver\", jdbc_driver)\\\n",
    "    .option(\"partitionColumn\", \"empd_id\")\\\n",
    "    .option(\"numPartitions\", 8)\\\n",
    "    .option(\"lowerBound\", 0)\\\n",
    "    .option(\"upperBound\", 8)\\\n",
    "    .option(\"dbtable\", \"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partitioning_unif = reader_partitioning_unif.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_partitioning_unif.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empd_id: int, emp_name: string, dept_id: int]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_partitioning_unif.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing to MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'mahfooz',1),\n",
    "        (2,'hamdan',1),\n",
    "        (3,'hadiya',2),\n",
    "        (4,'shaziya',2)\n",
    "       ]\n",
    "\n",
    "df=spark.createDataFrame(data,[\"empd_id\",\"emp_name\",\"dept_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"jdbc\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"url\", jdbc_url)\\\n",
    "    .option(\"user\", jdbc_user)\\\n",
    "    .option(\"password\", jdbc_password)\\\n",
    "    .option(\"ssl\", \"true\")\\\n",
    "    .option(\"driver\", jdbc_driver)\\\n",
    "    .option(\"dbtable\", \"emp\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push down a query to the database engine\n",
    "You can push down an entire query to the database and return just the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"(select * from emp where empd_id < 10)\"\n",
    "df = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", jdbc_url)\\\n",
    "    .option(\"user\", jdbc_user)\\\n",
    "    .option(\"password\", jdbc_password)\\\n",
    "    .option(\"ssl\", \"true\")\\\n",
    "    .option(\"driver\", jdbc_driver)\\\n",
    "    .option(\"query\", pushdown_query).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|empd_id|emp_name|dept_id|\n",
      "+-------+--------+-------+\n",
      "|      1| mahfooz|      1|\n",
      "|      2|  hamdan|      1|\n",
      "|      3|  hadiya|      2|\n",
      "|      4| shaziya|      2|\n",
      "+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from JDBC connections across multiple workers\n",
    "\n",
    "In order to read data in parallel, the Spark JDBC data source must be configured with appropriate partitioning information so that it can issue multiple concurrent queries to the external database. If you neglect to configure partitioning, all data will be fetched on the driver using a single JDBC query which runs the risk of causing the driver to throw an OOM exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empd_id: int, emp_name: string, dept_id: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "connectionProperties = {\n",
    "  \"user\" : jdbc_user,\n",
    "  \"password\" : jdbc_password,\n",
    "  \"driver\" : \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"emp\", column=\"empd_id\", properties=connectionProperties, lowerBound=1, upperBound=100000, numPartitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+\n",
      "|empd_id|emp_name|dept_id|\n",
      "+-------+--------+-------+\n",
      "|      1| mahfooz|      1|\n",
      "|      2|  hamdan|      1|\n",
      "|      3|  hadiya|      2|\n",
      "|      4| shaziya|      2|\n",
      "+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the JDBC fetchSize parameter\n",
    "\n",
    "JDBC drivers have a fetchSize parameter that controls the number of rows fetched at a time from the remote JDBC database. If this value is set too low then your workload may become latency-bound due to a high number of roundtrip requests between Spark and the external database in order to fetch the full result set. If this value is too high you risk OOM exceptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider the impact of indexes\n",
    "If you are reading in parallel (using one of the partitioning techniques) Spark issues concurrent queries to the JDBC database. If these queries end up requiring full table scans this could end up bottlenecking in the remote database and become extremely slow. Thus you should consider the impact of indexes when choosing a partitioning column and pick a column such that the individual partitions’ queries can be executed reasonably efficiently in parallel.\n",
    "\n",
    "**Make sure that the database has an index on the partitioning column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider whether the number of partitions is appropriate\n",
    "\n",
    "Using too many partitions when reading from the external database risks overloading that database with too many queries. Most DBMS systems have limits on the concurrent connections.\n",
    "\n",
    "As a starting point, aim to have the number of partitions be close to the number of cores or task slots in your Spark cluster in order to maximize parallelism but keep the total number of queries capped at a reasonable limit. If you need lots of parallelism after fetching the JDBC rows (because you’re doing something CPU bound in Spark) but don’t want to issue too many concurrent queries to your database then consider using a lower numPartitions for the JDBC read and then doing an explicit repartition() in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider database-specific tuning techniques\n",
    "The database vendor may have a guide on tuning performance for ETL and bulk access workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9545096d6a52d4b34adce3167e4787f469b0299861bb959c27dd86fc7feca162"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
